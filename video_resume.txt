00:00:04	vous en faites pas on va parler de ça dans une minute mais avant toute chose bonjour les data east et merci beaucoup merci parce qu'on vient de dépasser les 3000 abonnés sur cette chaîne youtube évidemment ça me fait très plaisir mais ce qui me fait encore plus plaisir c'est la communauté que vous êtes en train de construire je la trouve formidable vous pouvez me croire parce que je reçois régulièrement des emails que certains d'entre vous l'envoie et je peux voir dans ce que vous m'écrivez votre

00:00:36	motivation et votre votre intérêt dans le match in running je peux vous dire que ça va vous emmener très loin et je suis heureux de faire partie d'une communauté de gens aussi intéressé par le machine langue donc continuer comme ça et je vous remercie beaucoup pour votre soutien alors aujourd'hui je vous propose une vidéo poser sans gros montage pour pouvoir vous parler un petit peu plus en détails de la crosse validation donc vous vous rappelez on a parlé de la crosse validation dans la dernière vidéo python

00:01:09	spécial machine learning qui parlait de modèles sélection je vous avais dit qu'il existe plusieurs types de crosby et sean en à kayes fold on allait stratify decay fold on a aussi groupe caille fold donc ce que je vais faire dans cette vidéo c'est de vous montrer comment mettre en place ces différentes techniques dans python et je vais également utiliser un jeu de cartes pour pouvoir vous montrer pour pouvoir vous décrire ces différentes stratégies pour que vous puissiez bien comprendre comment sont découpés les données du

00:01:44	times est lorsqu'on utilise une crosse validation alors imaginez que ce paquet de cartes soit en réalité votre data 7 on a donc on va dire deux classes dans notre dataset une classe avec des cartes noir et une classe avec des cartes rouges au passage ici on a des cartes mais ça pourrait être tout et n'importe quoi ça pourrait être par exemple différentes à sion et certains ont malheureusement un cancer et d'autres n'ont pas de cancer ou bien ça pourrait être des emails et on a des emails qu'ils sont du spam et d'autres

00:02:14	qui ne sont pas des spams donc comme on l'a vu lorsqu'on fait du machine learning la première chose qu'on fait c'est qu'on découpe notre data 7 en 1 37 et un test 7 donc on avait vu qu'on dit coupé par exemple 20% dans le test 7 et les 80% restants dans le 3e set sauf que bien entendu si on découpe notre date assiette comme ça là on se retrouve avec uniquement des cartes rouges ce qui peut pas être une bonne chose c'est pour ça que ce qu'on fait toujours avant de découper c'est de mélanger notre jeu de cartes très

00:02:50	important donc on mélange un peu et ça ça se fait à travers l'argument shuttle qui lie à dans la fonction trim test split donc cet argument vous pouvez le trouver comme vous vous baladez dans ses équipes leur nez vous verrez que de base il a la valeur choo par défaut donc le mélange se s'effectuera toujours avant d'effectuer le split donc une fois qu'on a bien mélanger notre paquet de cartes on va le découper à la louche pour mettre 20 % ici et 80% ici et 20% là donc comme vous le savez les données du

00:03:26	tz7 on l'aimé toujours deux côtés on s'en sert uniquement pour l'évaluation finale du modèle quand on veut valider un modèle de machine learning on va prendre une autre partie de notre 3ème set et ça ça va s'appeler la validation 7 et comme on l'a vu dans la dernière vidéo la technique de crosse validation elle se dit et se trouve on va pas avoir de chance et dans ce test de validation on va encore une fois avoir que des cartes rouges donc la technique de crosse validation et consiste à dire que

00:03:57	on va faire plusieurs split par exemple 4 split 1 2 3 et 4 et on va successivement entraîné puis validé notre modèle sur s'explique par exemple on va entraîner une autre modèle sur ces trois groupes puis on va utiliser un autre split donc on va entraîner sur ces trois paquets puis on va évaluer sur ce dernier paquet puis on va encore alternée et ainsi de suite alors juste au passage j'ai une petite remarque à faire cette technique de crosse validation en général on la retrouve pas en diplôme ning et ceci pour deux raisons la

00:04:40	première c'est que si on a effectué une crosse validation c'est parce que si on reprend notre jeu de données ici on n'avait que des cartes rouges en diplôme ning com on a des millions de données à disposition ça veut dire qu'on aurait des millions de cartes dans ce petit paquet que vous voyez là donc parmi des millions de cartes si on en a pas quelques unes qui sont noirs c'est qu'on a vraiment pas de chance donc andy planning c'est inutile d'utiliser la crosse veille d'élections par souci de répartition de nos données ça c'est la

00:05:15	première raison la deuxième raison c'est que lorsqu'on fait une crosse de validation avec par exemple 4 split eh bien on va entraîner quatre fois notre modèle est entraîné un modèle de machine learning ça peut prendre des heures donc si on veut si on doit en plus m'entraîner quatre fois c'est vraiment pas top donc pour ces deux raisons vous verrez vraiment pas souvent des gens utiliser la crosse validation andy pleurs ni en général on se contentera d'un jeu de validation et d'un jeu de test et encore

00:05:49	dés fois dans certaines situations n'aura pas du tout de jeu de validation à présent cette petite parenthèse étant refermer je vais vous parler des différentes stratégies de crosse validation donc la première la plus connue la plus simple c'est la stratégie de kayes fold donc elle consiste à prendre notre notre 37 à commencer par le mélange et savourons le fais tout le temps et ensuite ce qu'on va faire c'est que on va découper 4 split quatre sections complètement égal et de façon complètement on va dire complémentaires

00:06:27	les unes après les autres exactement comme je viens de le faire sur le paquet de cartes ensuite en effet on entraîneur à notre modèle première fois là dessus et on les verra là dessus puis on va alterner avec les différentes possibilités exactement comme ce qu'on a vu avant pour faire ça dans python c'est vraiment très simple on commence par importer cas fold qui nous vient du module modèle sélection et une fois qu'on a ça on va écrire cv égale key fold entre parenthèses et là tout ce qu'on a à

00:06:58	faire c'est d'indiquer un nombre de split si on en veut 3 4 5 7 10 autant qu'on désire par exemple 5 c'est le plus courant on peut aussi indiquer le l'état aléatoire sur lequel on veut être au moment de mélanger notre data 7 avant la découpe comme d'habitude on est en écrivant rindom state égales par exemple 0 une fois qu'on a écrit cette ligne ensuite on peut utiliser la fonction cross valls corps dans laquelle on va par exemple faire passer nos données x y donc dans ce cas là j'ai les données du

00:07:37	data 7 des fleurs d'iris et ensuite on va écrire que notre cv et bien il va être égale ici en l'occurrence à notre valeur cvs le nombre de split va être égal à un littérateur de type k fold bien sûr on n'oublie pas décrire un modèle donc par exemple un modèle de kayl kneeboard classe i nférieure et alors on obtient 5 score pour nos cinq cross validation alors quand on utilise la technique de kayes fold il existe un cas extrême dans lequel le nombre de split va être égal au nombre d'échantillons qu'on a dans notre

00:08:13	troisième set et ça c'est ce qu'on appelle le livre one out ça veut dire en gros qu'on va prendre une carte on en sort une livre and out on va entraîner notre modèle sur toutes ses cartes et on va l'évaluer le valider le valider sur cette carte là une fois qu'on aura fait cette étape on va remettre cette carte dans le data 7 puis on va passer à la carte suivante et on va recommencer on entraîne là dessus on valide là dessus puis on passe encore une fois à la carte suivante donc on va faire autant d'entraînement

00:08:48	que l'on a deux cartes dans notre jeunesse est donc pour faire ça on pourrait bien sûr tout simplement remplacé cinq par la quantité de cartes que j'ai ici mais la meilleure façon de le faire c'est encore d'importer lives one out depuis le module modèle sélection et de remplacer caille fold par tout simplement un littérateur livre and out et donc si on compile ce qu au delà et bien voici le résultat qu'on obtient on a à chaque fois un score de 100 % ou 2 0% puisque on a toujours évolué sur une seule carte donc soit la prédiction

00:09:30	a été bonne sur sept cas soit elle a été mauvaise et on a autant de prédiction autant de ce corps si on puis dire que l'on a deux cartes dans notre data 7 alors bien sûr un gros des avantages de cette technique c'est qu'elle consomme énormément de puissance sur votre ordinateur là on a un tout petit dataset avec un petit modèle mais si vous avez un dataset avec des milliers voire des dizaines de milliers de données dedans ça va pas le faire clairement cependant c'est utile de connaître cette technique ne serait ce

00:10:03	que pour votre culture générale vous connaissez donc livoye note qui est un des cas particuliers de la technique cai fold maintenant il y a un léger désavantage à utiliser la technique key fold c'est que si vous êtes en présence deux dates à 7 avec des classes déséquilibrée par exemple que sur toutes ses cartes et bien seulement on va dire une toute petite portion appartient au son des cartes noir c'est-à-dire appartient à la classe 1 et toutes les autres appartiennent à la classe zéro alors si vous procédez aux cayes folle

00:10:38	de même après avoir mélangé vos cartes et bien vous risquez fortement de vous retrouver avec des segments c'est à dire des split dans lesquelles vous n'aurez pas certaines classes de certaines cartes où certains échantillons de la classe zéro ou de la classe ce1 donc typiquement quand vous serez dans des situations de classe déséquilibré il faudra utiliser la technique stratify d' key fold qu'on va voir dans une seconde mais j'ai envie de compléter en disant que cette technique de kai fu leur est vraiment très bonne

00:11:12	et elle est notamment très utile si vous voulez faire des régressions là où il n'y a pas de l'histoire de classe à avoir maintenant il existe une autre technique qui est une très bonne alternative aux cayes fold c'est le shuffle spitz alors ça ça consiste tout simplement à prendre notre 37 et à le mélanger puis à définir une portion de données pour le test par exemple ceci pour la validation et une portion de données pour l'entraînement on entraîne et puis on valide notre modèle puis on va regrouper

00:11:54	nouveau les données on va les remets langer si j'arrive à mélanger mes cartes et on va recommencer autant de fois qu'on le désire avec à chaque fois la même proportion par exemple de nouveaux 30% ici et 70% là etc etc donc pour faire ceci dans python on va importer cette fois-ci shuffle split comme ceux ci est ensuite à l'intérieur de notre cros validation va faire passer chef split et on va définir un nombre de spitz que l'on va voir par exemple ça peut tout simplement être 4 split cette fois ci pour changer

00:12:33	et ensuite on va définir une quantité une proportion qu'on veut avoir dans notre test saiz exactement comme quand on utilisait la fonction traint est speed voilà si on veut avoir deux pour 20% on va écrire 0,2 si on veut avoir 50% on va écrire 0.5 et c'est donc ensuite on écrit test size égales par exemple 0,2 donc si j'exécute ce code ça me donne 3 4 score puisque j'ai fait quatre fois cette cette opération de mélanger et découpé mélanger des coupes etc au passage on peut faire la même remarque sur cette technique que celle

00:13:16	que j'ai faite sur le cai fold c'est à dire que si vous êtes en présence d'un tête à tête avec des classes déséquilibré c'est à dire que dans toutes ces données admettons que seulement une toute petite partie de mes cartes faille font partie de la classe zéro et toutes ses cartes font partie de la classe 1 alors en mélangeant tout ça puis en découpant je risque de me retrouver avec un split dans lequel je n'ai aucune donnée de la classe zéro lors de l'entraînement auquel cas c'est pas bon du tout donc

00:13:51	pour éviter ces problèmes on utilise la technique stratify des cayes fold et cette technique c'est un petit peu le choix par défaut c'est un choix assez sûre il fonctionne en général très bien en machine learning comme vous pouvez le voir à l'écran ça consiste à créer des split dans lequel on va dans lesquels on va à chaque fois avoir une petite portion de chacune de nos classes donc vous voyez à l'écran on a une classe bleus une classe beige et une classe marron et pour le speed 0 on va prendre une petite portion de la

00:14:27	classe bleus une petite portion qui reste proportionnelle de la classe d'âge et pour finir une petite portion de la classe bruno donc si on voulait faire ça avec les cartes il faudrait d'abord créer ce qu'on appelle des strates à c'est-à-dire la machine va elle-même m trier nos différentes classes en se disant ok bon ben là on a toutes les routes ou les rouges et là on a tous les noirs voilà et une fois que c'est fait la machine va nous demander combien de crosse validation c'est à dire combien

00:14:57	de split on désire avoir dans nos deux classes et on va lui dire peut-être quatre puisque c'est le nombre qu'on a à l'écran 0123 donc la machine va découper sa en quatre portions 1 2 3 4 et des portions qui sont toutes égales et elle va faire la même chose pour les cartes de la classe zéro ou de la classe 1 c'est comme vous voulez enfin des cartes noire créée et ensuite elle va regrouper les deux ce qui nous donne nos quatre split de cross validation dans lesquels on va retrouver une certaine proportion des deux classes

00:15:35	la classe 1 et la classe zéro donc pour faire ça dans python on écrit stratify de kayes fold pour charger cette littérateurs puis on remplace cv remplace notre chauffeur speed par stratify day fold à l'intérieur duquel on va tout simplement indiquer le nombre des coupes de découpe que l'on désire par exemple 4 etc voici encore une fois on obtient 4 score pour finir je vais vous parler des groupes key fold cela ils sont très importants à connaître en machine learning en date à ces jeunes souvent statistiques on travaille très

00:16:12	souvent avec des dates à 7 dans lesquels nos variable ou nos échantillons sont tous indépendants les uns des autres et sont tirés de la même distribution si vous avez un dataset avec des appartements et le prix de ses appartements vous pouvez facilement dire que le prix d'un appartement ne va pas dépendre du prix d'un autre appartement c'est pas parce que vous avez un appartement à paris qui vaut cinq cent mille euros que forcément l'appartement qui aura à lyon il va lui coûter 200 mille euros non les deux choses sont complètement

00:16:45	indépendantes l'une de l'autre mais il arrive quand même en machine learning and data n 110 planning qu'on travaille sur un projet dans lequel nous donner dépendent les unes des autres par exemple on pourrait avoir un dataset qui analysent différents patient pour voir s'ils ont un cancer ou non mais on pourrait analyser ça selon les familles des gens est typiquement si on détecte que dans une même famille beaucoup de gens ont un cancer alors on va pouvoir considérer un facteur génétique donc c'est important de se dire à cette

00:17:23	personne a plus de chance étant donné qu'elle dépend de cette famille elle dépend de ce groupe cette personne a plus de chances de développer elle-même un cancer donc dans ces cas là il est très important de faire notre cros validation en considérant certains groupes qui on a un qui représente un facteur d'influencé dans notre data 7 par exemple on pourrait choisir de mettre de créer des groupes selon les les symboles en réalité c'est ça qu'on appelle une couleur sur un jeu de cartes par exemple

00:17:53	les trèfles les pics les quarts ou les coeurs et une fois qu'on a créé nos quatre groupes ensuite on va définir un nombre de split pour notre cross validation par exemple on va vouloir faire 3 split donc on va diviser chaque groupe en trois proportions égales voilà on va avoir trois groupes de trèfle trois groupes de pique trois groupes de carreaux et trois groupes de coeur et ensuite on va rassembler ces différents groupes pour créer nos différentes nos différents paquets de crosse validation par exemple on va

00:18:27	une fois m ce trèfle avec ce carreau ainsi que ce coeur ça ça va représenter notre validation set et ensuite on va entraîner tout le reste la machine surtout les données restantes puis validées sur le validation 7 à ce moment on aura simplement fait un spitz de notre cross validation pour passer au deuxième spit on reprendra ni nos différents groupes et on fera un mélange différents afin de couvrir toutes les configurations possibles alors pour faire ça avec python on va commencer par charger littérateurs

00:19:02	groupe key fold et vous au passage vous pouvez voir qu'il existera aussi groupe chaffrail speed pour encore une fois faire la même chose c'est-à-dire au lieu de créer des cayes fold on va créer nos groupes on va les mélanger on va créer ds lite puis on va rassembler nos groupes on va les mélanger on va recréer split et ainsi de suite donc nous ici on va choisir groupe cai fold et ensuite tout ce qu'on a à faire c'est à la place de cv on va écrire groupe cai fold on va indiquer le nom de le nom de split que l'on désire avoir enfin en

00:19:34	réalité le nombre d'entraînements que l'on désire avoir par exemple 5 l'entraînement puis une fois qu'on a écrit notre nombre de split on va fermer la parenthèse et on va utiliser une méthode qui s'appelle guette n splits à l'intérieur duquel on veut faire passer nos données x y puis on va faire passer les différents groupes qu'on veut avoir par exemple gloups égal alors bon là j'ai pas des très bons groupes parce que c'est mon dada 7 des fleurs d'iris mets typiquement là vous devriez faire passer

00:20:06	par exemple les différentes classes des passagers du titanic les gens de la classe numéro 1 feront partie du groupe numéro un les gens de la classe numéro 2 feront partie du groupe numéro deux et c'est donc ce que je peux faire ici c'est écrire group's égale x dû par exemple de la variable 0 donc une de mes colonnes demande à tassette des fleurs d'iris et donc si j'exécute ce code ça me donne les résultats de crosse validation suivant voilà c'est la fin de cette vidéo j'espère qu'elle vous a plu

00:20:39	qu'elle vous a permis de mieux comprendre la crosse validation si ce format de vidéo vous a plu merci de me le dire dans les commentaires et le fait de produire des vidéos live sans gros montage ça me permet de produire plus rapidement des vidéos pour vous apporter de la valeur donc c'est pour ça que je pense que c'est une bonne idée d'alterner entre grosses vidéos et vidéo un petit peu plus soft si vous avez des questions merci de les laisser en commentaire et moi je vous dis à très vite pour la prochaine vidéo

00:21:09	[Musique]



00:00:00	bonjour les datasite east dans cette vidéo on va parler de metric et plus précisément de metric régression en effet vous êtes nombreux à vous demander quelle est la différence entre la rse la mae le coefficient air carré et dans quelles situations utiliser l'un plutôt que l'autre donc aujourd'hui je vais répondre à toutes ces questions en partageant au passage quelques techniques issues de ma propre expérience c'est parti pour bien comprendre l'origine de nos métriques il faut repartir de la base voici donc

00:00:36	un data 7 x y à partir duquel est développé un modèle de régression linéaire par exemple il pourrait s'agir ici du prix de plusieurs appartements en fonction de leur surface habitable pour évaluer la performance de ce modèle il faut mesurer les erreurs entre ces prédictions et les valeurs du date 7 on calcule ainsi la différence entre y vrai et y pred et de cette façon si votre modèle estime qu'un appartement vos trois cent mille euros mais que sa vraie valeur et de 250000 euros alors vous pouvez dire qu'il a fait une

00:01:14	erreur de moins 50000 euros ici l'erreur est négative mais en prenant un autre exemple il se pourrait très bien que l'erreur soit positive or travailler avec des erreurs qui peuvent être soit positive soit négative ce n'est pas vraiment pratique et je vous laisse deviner pourquoi dans les commentaires de la vidéo donc pour corriger ce problème il y a deux options prendre la valeur absolue de chaque erreur ou bien calculé le carré de chaque erreur et puis comme vous le voyez l'erreur et du coup beaucoup plus grande

00:01:50	mais on verra dans quelques instants comment remettre ses erreurs à leur échelle initial ensuite une fois qu'on a choisi une de ces deux approches il ne reste plus qu'à faire la moyenne de toutes les erreurs donc quand on fait la moyenne des erreurs quadratique on obtiendra l'erreur quadratique moyenne ce qui s'appelle minsk ward error en anglais et quand on fait la moyenne d erreur absolue on obtient l'erreur absolue moyenne ce qui s'appelle en anglais mines absolut erreur alors pour calculer ses erreurs avec ce kit l'orne

00:02:26	il faut commencer par les chargés depuis le module metrix au passage vous remarquez qu'on utilise ses fonctions toujours de la même manière en passant dans les parenthèses y puis y prennent donc j'ai ici créer deux tableaux y est y bred qui sont pour le moment parfaitement identiques et j'ai également importer tout le contenu du module metrics donc l'erreur absolue moyenne l'erreur quadratique moyenne et d'autres erreurs qu'on va voir dans cette vidéo ainsi je peux calculer la mine absolute erreur entre eux y est y près d'1 6 que

00:03:03	l'erreur quadratique moyennes mines square d'erreur entre nos deux tableaux ce qui nous donne pour le moment une erreur égal à zéro puisque nos deux tableaux sont identiques à présent si je modifie le contenu d'un des tableaux par exemple y prennent en écrivant 2 à la place de 1 alors je vais obtenir une différence entre 1 et 2 qui est égal à 1 donc dans les deux cas j'obtiens une erreur égal à 1 puis si je continue que je passe à 3 alors je vais obtenir un écart de 2 qui lorsqu'il est mis au carré me donne 4 et ainsi de suite si je

00:03:38	passe à 4 je veux obtenir une un écart de 3 s'il est mis au carré ça nous donne 9 puis si je passe à 5 on va du coup obtenir 16 vous comprenez etc etc alors à ce stade on pourrait penser que l'erreur quadratique moyenne c'est juste le carré de l'erreur absolue mais il faut surtout pas penser ça regardez les formules qu'on a ici l'erreur quadratique moyenne c'est la moyenne des erreurs carré et non pas le carré de la brienne des erreurs ce qui fait que si on ajoute une autre valeur dans notre tableau par exemple on

00:04:12	rajoute la valeur deux et on va tout simplement rajouté la même valeur donc on n'aura pas d'erreur sur cette deuxième prédictions alors on obtient une erreur absolue moyenne qui est égale à deux puisque ladite puisque la moyenne entre 4 et 0 ces deux et on obtient une erreur quadratique moyenne égale à 8 puisque la moyenne entre 16 et 0 7 égale à 8 hors 8 c'est pas le carré de deux ans fait la racine carrée de 8 ses bas ses deux racines de 2 et si on veut calculer cette racine carrée c'est très simple on

00:04:44	utilise la fonction racine carrée depuis le module num vaille ce qui nous donne donc 2,90 et ça ça porte un nom c'est ce qu'on appelle la route minsk ward erreur c'est à dire la racine carrée de l'erreur quadratique moyenne et ça c'est une métrique très utile car elle permet de remettre à l'échelle initial nos erreurs calculé par l'erreur quadratique moyenne donc avec tout ça on a d'un côté la mine absolute erreur qui dit que la moyenne entre 0 et 4 c'est égal à 2 ce qui est parfaitement logique et d'un autre côté

00:05:19	on a la mine square erreur pour qui cette moyenne est égal à 2,8 et ça ça peut sembler un peu plus bizarres donc ça nous amène à la question que beaucoup de gens se posent dans quelles circonstances utiliser la m est ce plutôt que la mae et bien la réponse est simple en général on utilise la m as eu quand on accorde une importance exponentielle à nos erreurs je vous donne un exemple imaginer qu'on développe un système qui estime la distance de freinage d'une voiture pour ça on entraîne deux modèles

00:05:57	dans le but de retenir celui qui fera les plus petites erreurs sur ses prédictions le modèle a fait une erreur de 10 mètres et une autre de zéro m donc si nous évaluons ce modèle avec la m à eux il obtient une erreur moyenne 2,5 mètres le modèle b quant à lui effectué l'une est rare de 6 mètres et une autre de 5 mètres ce qui lui vaut une moyenne de 5,5 m c'est donc le modèle à qui l'emportent mais est ce une bonne chose ce modèle a tout de même fait une erreur de 10 mètres ce qui est très grave parce

00:06:33	que sur une telle distance une voiture a beaucoup de chance de faire un accident ou d'écraser un piéton donc par curiosité voyons ce qui se passe en évaluant nos modèles avec la m est ce cette fois le modèle à obtient une est rare de 7 mètres tandis que le modèle b obtient une erreur de 5,52 m c'est donc le modèle b qui l'emportent dans cette situation ce qui explique une telle différence c'est que la ms feu pénalise beaucoup plus et grande erreur que la m hab en effet faire une erreur de 10 m ce n'est pas juste dix fois plus grave

00:07:11	que faire une erreur de à m mais ça devient 100 fois plus graves puisque le carré de 10 c'est égal à 100 donc en général il est conseillé d'utiliser la haye mais ceux ci vous accorder une grande importance aux grandes erreurs cela peut être crucial dans le réglage des hyper paramètres d'un modèle avec par exemple grid search cv al'inverse si les grandes erreurs que vous obtenez sont provoqués par des valeurs aberrantes ce qu'on appelle en anglais des haltes liars alors il vaut mieux se tourner vers la mae

00:07:47	qui vous donnera une meilleure représentation de la performance de votre modèle et puisqu'on parle de valeurs aberrantes il existe une autre métriques très utile car elle est encore moins sensibles à ces valeurs c'est la médiane absolut et or comme son nom l indique cette mesure retourne la médiane de toutes nos erreurs donc dans le cas où on a des erreurs qui sont un 2,8 3,1 alors en triant ses erreurs dans l'ordre on va voir que la médiane est de 2 donc l'erreur médiane absolue est égal à 2 ça

00:08:22	peut donc être très utile si vous ne voulez pas évaluer votre modèle sur quelques valeurs aberrantes qui pourraient exister par exemple si je continue d'ajouter quelques valeurs dans mes tableaux et que je rajoute une valeur vraiment très grande une très mauvaise prédiction par exemple une prédiction 2000 alors que la valeur était de 2 alors je vais obtenir des erreurs quadratique moyenne et d erreur absolue moyennes qui sont très très grande on pourrait donc penser que notre modèle effectué de très grosses erreurs alors

00:08:53	que la réalité est différente notre modèle n'effectuent pas beaucoup de grandes erreurs il effectue seulement une très grande erreur donc en calculant la médiane absolute error on va découvrir que notre modèle effectué en réalité une erreur de 2 alors est ce que c'est une bonne chose de dire à vos collègues votre client votre patron que votre modèle fait une erreur moyenne de dison 2 euros alors que il peut arriver qu'ils vous fassent perdre des milliers d'euros pas vraiment donc à cause de ça beaucoup de gens se demandent mais

00:09:27	qu'est ce que je suis censé utiliser entre la médiane absolute erreur l'erreur quadratique moyenne l'erreur absolue moyenne mais j'ai envie de vous dire la réponse est très simple pourquoi chercher à utiliser une seule mesure quand vous pouvez tout les utiliser à la fois c'est vrai en fin de compte lorsque vous calculer toutes les erreurs qu'un modèle fait pourquoi ne pas utiliser différentes statistiques pour bien comprendre ses erreurs moyenne la médiane et pourquoi pas les quintile voir la distribution que

00:10:01	suivent toutes ces erreurs vous pouvez ainsi déterminer des intervalles de confiance en disant à votre patron vos collègues que votre modèle a peut-être 5% de chance d'effectuer une erreur supérieure à 5000 euros ou 10000 euros dans le cas d'un prix d'un appartement et c'est ça qu'on doit faire lorsqu'on utilise des métriques de régression pour vous donner un exemple concret j'ai ici chargé le data 7 du prix de l'immobilier à boston à partir duquel j'ai développé un modèle de régression linéaire et voici le

00:10:36	résultat que l'on n'obtient et bien si j'ai envie de comprendre quelle est la performance de ce modèle je vais pas me contenter de calculer l'erreur moyenne non à la place je vais calculer la médiane le quintile et comme je disais pourquoi pas même la distribution donc ce que j'aime beaucoup faire quand j'ai besoin d'évaluer un modèle ces deux tracés l'histogramme de mes erreurs en calculant par exemple la valeur absolue entre y est y près d' jusque là ça ressemble à notre erreur moyenne absolue

00:11:08	sauf que plutôt d'en faire une moyenne je vais tout simplement afficher sa dans un histogramme avec marc lieb en créant par exemple disons 50 intervalles j'obtiens alors le graphique suivant sur lequel je peux voir que sur toutes les prédictions qui a été fait 80 d'entre elles ont une erreur très proche de zéro je peux également visualiser ou même calculer la quantité d'erreurs qui son supérieur à 5000 $ donc et grâce à cette histogramme je comprends beaucoup mieux l'état actuel de mon modèle sa performance en tout cas

00:11:44	beaucoup mieux que si je calcule et une simple moyenne ça c'est sûr et pour conclure avec cette histoire âme on peut voir que nos erreurs suivent une loi de distribution exponentielle c'est une chose tout à fait normal que vous obtiendrez la quasi totalité du temps voilà donc retenez bien mon conseil ne vous limitez pas à une métrique utilisé dans plusieurs vous récolterez beaucoup plus d'informations maintenant pour finir cette vidéo nous allons voir une toute dernière métriques le coefficient de détermination r carré

00:12:17	ce coefficient vous l'utilisez à chaque fois que vous écrivez modèle points score x y en tout cas pour les modèles de régression eh oui que ce soit un modèle de régression linéaire sgd récré sort ou même un réseau de neurones dans ces kits learn c'est toujours le coefficient de détermination qui est implémenté dans la méthode score alors vous le savez sans doute ce coefficient quand il est proche de 1 c'est que notre modèle est très bon quand il est proche de 0 sec le modèle est pas bon du tout

00:12:50	mais pour mieux comprendre ce qu'il représente vraiment voyons ensemble sa formule mathématique oui je sais ça fait un peu peur mais croyez moi c'est très simple à comprendre en fait le coefficient de détermination évalue la performance de notre modèle par rapport au niveau de variation présent dans les données regardez bien au numérateur on retrouve une expression très semblable à notre erreur quadratique on à la somme des carrés des différences entre y est y pred c'est donc une erreur qui est

00:13:25	calculé dans la partie numérateur de notre fraction ensuite au dénominateur aux calculs presque la même chose mais cette fois ci en faisant la différence entre y est la moyenne d y est ça en statistiques ça porte un nom c'est la variance c'est pourquoi on retrouve la phrase qu'on a écrit tout en bas de l'écran le coefficient de détermination évaluer les erreurs ou bien la performance par rapport aux niveaux de variation présent dans les données lorsque le coefficient de détermination est proche de 1

00:14:02	alors cette fraction est elle-même proche de zéro ce qui signifie que les erreurs commises par votre modèle sont beaucoup plus petite que la variance présente dans les données pour vous donner un exemple si vous développez un modèle sur des prix d'appartements que l'erreur moyenne de ce modèle et 2000 euros et que la variance dans votre data 7 est environ 200000 euros alors on obtient à ras bord de 1% est donc un coefficient de détermination égal à 0 99 al'inverse on obtient un coefficient de détermination égal à zéro lorsque le

00:14:44	numérateur est égal au dénominateur c'est à dire que les erreurs sont aussi grandes que le niveau de variation des données et dans le cas où vos erreurs sont encore plus grandes que la variance de votre date à 7 alors vous pouvez même obtenir un coefficient de détermination négatif donc dans notre cas avec le dataset de boston on obtient un coefficient de détermination de 0,60 14 qu'est ce que ça veut dire eh bien ça veut dire que notre modèle décrit 74% des variations du prix de l'immobilier boston c'est avec ce genre de phrase

00:15:27	que vous pouvez interpréter les résultats du coefficient de détermination encore une fois ce n'est pas une métrique à utiliser tout seul il faut l'utiliser en combinaison avec la haye mais cela m a eu tout ce qu'on a vu dans cette vidéo et si vous voulez utiliser ces différentes métriques dans une crosse validation alors il vous faut écrire pour l'argument scoring une chaîne de caractères contenant vos différentes métriques que vous pouvez trouver à l'adresse suivante je vous mets le lien dans la description si jamais vous

00:16:00	perdez cette page j'ai quand même une astuce à vous donner écrivez n'importe quoi compiler votre code et vous allez obtenir une erreur vous disant que la cette métrique n'existe pas bien évidemment mais au bas de cette erreur on vous dira de copier/coller le code suivant donc en importance et qui l'ornent point metrix ensuite on est capable d'exécuter cette ligne est alors vous obtenez la liste de toutes les métriques qui sont disponibles en tant que chaîne de caractères donc par exemple on va retrouver la mine

00:16:34	absolute erreur qui est formulée de façon négative mais c'est juste un détail vous en faites pas et donc en revenons tout en haut et en remplaçant ceci par notre minsk ward error on obtient les résultats à suivre donc sur trois crosses validation obtient une erreur absolue moyenne de 3,3 4,27 et 13,4 vous pouvez utiliser exactement cette même technique dont les fonctions grid search cv ou toute autre fonction de sa kittler qui utilisent la crosse validation voilà j'espère que cette vidéo sur les métriques vous aura plu

00:17:13	d'ailleurs je suis assez curieux de savoir quand est ce que vous vous préférez utiliser l'erreur clinquante moyenne ou bien l'erreur absolue moyenne et si vous aussi vous avez dit technique personnelle que vous voudriez partager n'hésitez pas à le faire dans les commentaires en dehors de ça si vous avez d'autres questions sur la date à sainz n'hésitez pas rejoindre toute notre communauté sur discorde et si la vidéo vous a plu merci d'alic et merci car partager avec un ami merci de vous abonner si ce n'est pas déjà fait quant à moi je

00:17:41	vous dis à très vite pour la prochaine vidéo [Musique] m [Musique]


00:00:00	bonjour et bienvenue dans cette 22e vidéo de la série python spécial machine learning aujourd'hui nous allons parler de dette a pris processing pour être tout à fait franc avec vous si vous ne maîtrisez pas ce sujet alors vous ne serez jamais un bon d'être scientist tout simplement parce que le deuil tu as pris processing est l'une des étapes les plus importantes pour développer des modèles avec de bonnes performances donc dans cette vidéo on va commencer par voir ce qu'est le delta prix pricing

00:00:29	je vous montrerai les différentes techniques à connaître puis nous verrons comment les mettre en place avec 5 it l'orne et comment construire une chaîne de transformation avec la classe pipeline du celtic l'orne allez c'est parti comme vous le savez les algorithmes de machine learning apprennent à partir des données qui leur sont fournies par conséquent si ces données sont de mauvaise qualité elles sont erronées ou incomplètes redondantes alors l'algorithme qui en résultent sera lui même assez mauvais puisqu'il est censé

00:01:08	refléter ce qu'il voit dans les données c'est pour cette raison qu'il est impératif de bien préparer nous donner avant leur passage dans la machine il faut les nettoyer les filtres et les normaliser et c'est cette étape qu'on appelle le prix pro ce seigneur ou en français le pré traitement des données parmi les opérations de prix processing les plus importantes on trouve l'encodage qui consiste à convertir les données qualitatives en valeur numérique la normalisation qui permet de mettre sur une même échelle

00:01:40	toutes les variables quantitative ce qui facilite beaucoup l'apprentissage de la machine l'imputation qui permet de remplacer les données manquantes par certaines valeurs statistiques la sélection de variables qui utilisent les tests statistiques comme le test de chi 2 pour sélectionner les variables les plus utiles au développement d'un modèle et l'extraction de caractéristiques qui consiste à générer de nouvelles variables à partir d'informations caché dans le data 7 pour effectuer toutes ces opérations c'est

00:02:13	kit l'orne ont développé différents modules un module de prix pricing qui permet d'effectuer des transformations d'encodage de normalisation et quelques autres opérations que nous verrons dans cette vidéo un module input pour les opérations d'imputation un module fischer sélectionne et pour finir un module fischer extraction dans cette vidéo on va se concentrer exclusivement sur le module prix preuve ce signe parce qu'il contient les opérations les plus importantes à connaître en machine learning

00:02:45	mais ne vous en faites pas on verra le contenu des autres modules dans une prochaine vidéo alors n'hésitez pas à vous abonner si vous ne voulez pas la louper allez c'est parti pour le module prix processing [Musique] dans le module prix processing de s'acquitte l'orne on retrouve deux choses d'un côté on a des classes appelé transformer ou transformateurs en français qu'on peut facilement repérer puisqu'elles commencent toutes par une majuscule et d'un autre côté on a de simple routine ou fonctions

00:03:19	mathématiques ce qui nous intéresse le plus ce sont ces transformer parce qu'ils offrent une interface de programmation est extrêmement utile pour faire du prix processing en effet l'interface permet de traiter l'ensemble de nos données de façon cohérente en transformant tout donné futur de la même manière qu'on était transformé les données qui ont servi à l'entraînement de la machine cela permet au modèle de machine learning qui vient après le transformer de bien fonctionner car il reçoit toujours des données

00:03:53	cohérentes avec ce qu'il a appris alors pour transformer ces données de façon cohérente les transformers dispose de deux méthodes une méthode fit qui permet de développer une fonction de transformation en analysant les données du trail 7 ainsi qu'une méthode transforme qui permet d'appliquer cette fonction de transformation sur toutes les données qu'on lui fournit c'est à dire aussi bien les données du trinket que les données du tz7 ou bien les données futur pour faire d'une pierre deux coups il existe une troisième

00:04:28	méthode appelée fit transforme qui combine la méthode fit avec la méthode transforme donc dans la pratique quand on désire développer un modèle de machine learning on commence par diviser notre data 7 en deux parties 1 37 et un test 7 avec les données du train 7 nous développons une fonction de transformation ce qui permet de traiter nos données pour ensuite entraîné un estimateur une fois cette étape terminée nous pouvons utiliser le transformer et l'estimateur tels qu'ils ont été développés pour

00:05:11	transformer les données du tz7 puis vers de nouvelles prédictions en combinant ainsi un transformers avec un estimateur nous obtenons une pipe line c'est à dire une chaîne de transformation je vais vous en parler un peu plus tard dans cette vidéo mais en attendant je vous propose d'entrer dans la pratique et de découvrir les différents transformer du module prix processing pour commencer nous allons voir ensemble les transformers d'encodage quand on développe un modèle de machine learning ou de diplômes ning il est nécessaire de

00:05:52	présenter à la machine des valeurs numériques avec lesquels elles puissent faire des calculs par conséquent si notre data 7 contient des données qualitatives sous forme de mots alors il est indispensable de convertir ces données en valeur numérique et c'est ça ce qu'on appelle l'encodage dans ces kits l'orne il existe cinq transformer d'encodage leibl enhco d'heure leibl parinaud riser multilabel payne riser ordinale enhco d'heure et one drop in codeurs c'est transformer permettent d'effectuer deux

00:06:30	type d'encodage à savoir l'encodage ordinale et l'encodage one drop l'encodage ordinale consiste tout simplement à associer chaque catégorie d'une variable a une valeur décimales unique par exemple la catégorie chat est associé à 0 la catégorie chiens à un oiseau à 2 etc etc pour ça on dispose des transformers leibl enhco d'heure et ordinale enhco d'heure le transformer leiber enhco d'heure a été conçu spécialement pour encoder la variable y c'est à dire qu'en principe ce transformers ne permet que de traiter

00:07:13	une seule colonne pour s'en servir il faut commencer par créer un objet de la classe leibl enhco d'heure puis cet encodeur il va falloir le développer grâce à la méthode fit dans laquelle on va faire passer notre tableau y cela retourne donc une fonction de transformation à ce stade on peut donc utilisez notre transformer pour traiter nous donner avec la méthode transforme et donc on obtient 0,1 0,2 comme on l'a vu tout à l'heure pour aller plus vite on peut également utiliser la méthode fit transforme à présent pour certains

00:07:52	transformers dont le transformer level and other il existe une méthode in verse transforme qui permet d'appliquer la transformation dans le sens inverse c'est à dire dans ce cas que ça nous permet de décoder nous donnait donc par exemple si on a entraîné un modèle de machine learning à identifier des animaux chats chiens oiseaux mais que ce modèle nous donne des valeurs numériques 012 et que nous on sait pas vraiment ce que c'est et 0 1 ou 2 donc on a besoin de savoir à quoi ça correspond eh bien on va utiliser notre encodeur

00:08:29	avec la méthode in verse transforme dans lequel on pourra faire passer par exemple un tableau nîmes paille qui va contenir par exemple 0 et 0,2 1,2 et ça ça va nous retourner cha cha oiseaux oiseaux voilà pour le transformer leiber enhco d'heure à présent si on désire encoder un tableau qui contient plusieurs variables le transformer combien d'utiliser la berline coda ne va pas fonctionner démonstration ci ont créé un objet leibl enhco d'heure et qu'on veut utiliser fit transforme sur le tableau x ça va nous retourner une

00:09:06	erreur qui nous indique que les dimensions ne sont pas adaptés à cette fonction donc à la place d'utiliser livelink odeurs il faut ici utilisé le transformer ordinale hangover qui fait exactement la même chose que les blenders sauf qu'il est conçu pour traiter les données x c'est-à-dire plusieurs variables à la fois donc ici le principe est le même on va simplement remplacer lemon codeurs par ordinale heineken et cette fois ci il en obtient le résultat suivant bon maintenant l'inconvénient avec cette technique dont

00:09:39	codage c'est que d'un point de vue arithmétique ça reviendrait ici et à dire qu'un chat c'est inférieur à un chien qu'un chien c'est inférieur à un oiseau et bien sûr dire ce genre de choses ça n'a absolument aucun sens car ces catégories n'ont rien d'original cela risque donc de pénaliser la plupart des modèles de machine learning à l'exception des modèles qui sont basés sur les arbres de décision car cela ne sont pas sensibles aux relations d'ordre dans les données qu'on leur présente donc pour éviter ça il existe un autre

00:10:16	type d'encodage l'encodage one shot avec cette technique chaque catégorie ou classe a est représenté de façon binaire dans une colonne qui lui est propre on décompose ainsi la variable initiale en plusieurs sous variable créant donc autant de colonnes que l'on a deux catégories ou de classe dans cette variable de cette manière les algorithmes de machine learning ne peuvent plus comparer les valeurs chats chiens et oiseaux sur un seul et même axe car ces catégories sont désormais séparés en plusieurs dimensions

00:10:54	alors pour faire sa danse acquis de l'orne on dispose de trois transformer le transformer l'île bagne riser multilevel parinaud risers et onenote un codeur démonstration si cette fois ci nous créons un objet l'ibl bein riser que nous utilisons donc la méthode fit transforme pour traiter nous donner y on obtient le tableau sur maintenant beaucoup de gens pensent que locaux d'ajoie note présente lui aussi un inconvénient en effet dans le cas où nous avons un très grand nombre de catégories par exemple plusieurs centaines de

00:11:32	villes dans un data 7 d'immobilier alors le résultat de l'encodage winehouse va nous donner un tableau extrêmement large et difficile à manipuler mais en réalité ce n'est pas vraiment un problème parce que ce tableau bien qu'il soit très large ne va pas peser très lourd sur la mémoire de trois ordinateurs la raison à cela est que nous sommes ici en présence d'une matrice creuse en anglais soir semailles tricks c'est à dire une matrice qui est remplie en grande majorité de nombreux 0 celle ci peut donc être présenté dans un

00:12:10	format très léger dans lequel le chiffre zéro mais tout simplement pas stockées dans la mémoire de l'ordinateur puisqu'il ne sert à rien par exemple si nous avons la matrice cinq par cinq suivantes nous pouvons stocker cette matrice dans trois tableaux plus petit un tableau pour enregistrer les valeurs n'ont nulle c'est à dire à b et c un tableau pour enregistrer les lignes de ses valeurs et un tableau pour enregistrer les colonnes de ses valeurs par exemple à est à la ligne zéro et la colonne 0 b quant à lui est à la ligne 1

00:12:47	que la deux et la valeur c était la ligne 3 colonnes quatre toques on écrit 3,4 au final on a pu stocker cette matrice dans trois tableaux qui contiennent trois valeurs c'est un peu l'équivalent d'avoir une matrice trois par trois qui stocke le contenu d'une matrice cinq par cinq ce format de stockage est appelé le format coordinator c o o ce n'est pas le format qui va être utilisé en s'acquitte langue parce que celui qui est utilisé dans ces kits langue c'est le faut c'est le format csr pour compressed sparrow

00:13:22	c'est un format un petit peu plus que brasser mais qui ressemble beaucoup à celui là ainsi quand vous effectuez un encodage one out avec ça qu'ils donnent vous avez l'option de choisir si votre résultat doit être compressé ou non pour ça tout ce qu'on a à faire dans la fonction ou dans la classe l'abeille noire âge là c'est d'écrire ce parce aux égales par exemple auquel cas on obtient donc une matrice 4 par 3 qui a été compressé en tant que choriste ce père héros comme je vous le disais donc le format csr et

00:14:00	pour le transformer one drop un codeur c'est même le choix par défaut puisque en principe si vous effectuez une opération d'encodage en boîte à outils sur toutes vos variable x vous allez vous retrouver avec un tableau très très large donc s'ils quittent l'orne de base décide de compresser votre résultat avec un format csr donc on peut le voir si vous faites fi tu transformes de notre tableau x on obtient par défaut une matrice qui est compressé en csr voilà c'est tout pour les encodeurs mais avant

00:14:36	de passer à la suite de cette vidéo j'ai une remarque importante à faire aujourd'hui dans la version 0.22 le s'acquitte l'orne c'est transformer ne sont pas capables de gérer l'apparition de nouvelles catégories dans vos données c'est à dire qu'il retourne une erreur quand ils sont confrontés à une catégorie qu'ils n'ont jamais vues dans le trend 7 en principe si on travaille proprement ce genre d'erreur ne doit pas arriver mais bon on sait jamais en date à saint helens on est souvent amené à s'adapter

00:15:07	aux problèmes sur lesquels on travaille c'est pourquoi pas mal de data insiste préfère créer leurs propres fonctions d'encodage plutôt que de dépendre des fonctions de c-kit l'orne néanmoins les transformers convient de voir sont extrêmement utiles et beaucoup de gens les utilisent malgré ce petit défaut qui peut être un jour sera corrigée voilà donc ce que vous devez retenir sur l'encodage avec cinq langues à présent je vais vous parler d'une autre opération essentielle en machine au rni et en data signs peut être même

00:15:41	la plus importante la normalisation en date à sainz il est indispensable de normaliser nous données quantitatives c'est à dire les mettre tout sur une même échelle cela facilite considérablement l'apprentissage des modèles de machine learning qui sont basés sur la descente de gradient les calculs de distance où les calculs de variance par exemple dans le cas d'une descente de gradient lorsqu'une variable prend le dessus sur une autre alors il devient plus difficile pour la fonction coup de converger vers son minimum c'est

00:16:21	pourquoi il est très important de normaliser nos données avant leur passage dans la machine pour ça il existe beaucoup de technique de normalisation et ensemble nous allons voir les trois plus connus à savoir la normalisation minimax la standardisation ainsi que le transformer robuste skyler ne s'acquittent langues la normalisation minimax consiste à transformer chaque variable de telle sorte à ce que ces valeurs soient toutes comprises entre 0 et 1 pour ça on soustrait chaque valeur d'une variable au minimum de cette variable

00:17:00	puis on soustrait par l'écart entre le maximum de la variable est le minimum de la variable par exemple si on a les valeurs suivantes 70 80 120 les valeurs qui pourraient par exemple être des surfaces d'appartements alors en créant tout simplement un skyler de la classe mini max skyler on va obtenir les résultats suivants donc zéro 0,2 et 1 alors vous allez vous dire mais c'est bizarre si on déforme ainsi nos données bah ça veut plus rien dire notre modèle va rien comprendre père de l'information

00:17:37	sur nos véritables surface et bien en réalité non on ne perd aucune information car les normalisations ont conservé les rapports de distance qu'il y avait dans nos données c'est à dire que l'écart qu'il ya entre 0 et 0,2 et complètement équivalent à l'écart qu'il ya entre 70 et 80 pour notre machine ça ne change absolument rien bien sûr quand on voudra utiliser notre modèle sur de nouvelles données par exemple sur un appartement de 90 mètres carrés alors il faudra normalisées ces données en y appliquant

00:18:13	la même transformation c'est à dire que dans la méthode transforme de notre skyler on va en réalité trouver le calcul suivant pour être honnête avec vous si vous avez compris ça si vous comprenez ce calcul alors vous avez tout compris au concept de transformer et vous avez tout compris à cette vidéo sinon si vous avez un doute n'hésitez pas à poser vos questions dans les commentaires je ferai de mon mieux pour que tout le monde comprenne bien ça parce que c'est fondamental bref revenons à notre code

00:18:43	pour bien visualiser l'effet de la normalisation mais max je vous propose de charger le dataset des fleurs d'iris donc vous connaissez déjà ce dataset il nous donne le nuage de coin suivante donc si on voulait normalisé nous donner tout ce qu'on va faire c'est qu'on va faire x mines max qui est égal à donc le mini max skyler qu'on va entraîner sur nos données x et donc après normalisation on obtient le résultat sur donc on arrive à ne les données originales qui sont ici on a les données qui ont été transformés qui sont là et

00:19:24	on peut voir que toutes nos données sont à chaque fois entre 0 et 1 à présent une autre technique de normalisation très utilisée en data sérieuse c'est la standardisation cette technique consiste à transformer nos données de telle sorte à ce que chaque variable est une moyenne égale à zéro et un écart-type égal à 1 pour ça il faut soustraire chaque valeur à la moyenne initiale de notre variable est divisé le tout par l'écart type initiale de notre variable on obtient ainsi des données simple à utiliser pour la plupart des

00:20:03	modèles statistiques comme les supports de vector machine ou bien les décompositions principle component analysis donc si on reprend l'exemple précédent avec les données 70 80 120 ans remplaçant mines max skyler par un standaard skyler on obtient - 0,92 -0 46 et 1,38 à présent si on compare cette technique avec celle qu'on a vue auparavant sur le data 7 des fleurs d'iris alors on obtient un résultat qui ressemble à ceci donc nous donner ne sont pas aussi écraser que ce qu'on pouvait avoir avec mini max on a une moyenne de toutes nos

00:20:49	données qui est égal à zéro et on avait un écart type pour chaque variable qui est égal à pain ces deux techniques sont très bien on les utilise très souvent en machine learning néanmoins ces deux techniques ont un gros inconvénient c'est qu'elles sont sensibles aux valeurs aberrantes qui pourrait y avoir dans nos données c'est à dire les oilers pour vous montrer ça si on rajoute quelques valeurs aberrantes à la fin de notre data 7 disons 10 valeurs qui sont tous aux alentours des 100 alors on obtient le

00:21:20	nuage de points suivants on pourrait se dire waouh mais qu'est-ce qui se passe et bien c'est juste que ici il y à mes valeurs aberrantes et ici si je zoome on va retrouver le dataset des fleurs d'iris ça c'est le ce qu'on est censé avoir et ce qu'on a ici là haut ce sont nos valeurs aberrantes et bien si nous effectuons une normalisation en prenant en compte ces valeurs aberrantes on va se retrouver avec le résultat suivant les données qui en subit la normalisation mines max ici en orange et qui sont donc censés être toutes

00:21:53	comprises entre 0 et 1 se retrouvent écrasé par la présence d out liars cela les rend donc vraiment très difficile à exploiter pour créer un modèle de machine learning et on peut voir qu'on a exactement le même problème pour les données qui ont subi la standardisation celles ci sont devenues très difficiles à exploiter car elle se retrouve extrêmement compressé pour obtenir une moyenne égale à zéro et un écart-type égal à 1 en conciliant les hotlines ici présents conclusion ces deux techniques de normalisation ne sont pas efficaces

00:22:29	lorsque nous avons des valeurs aberrantes dans un data 7 heureusement dans cette situation nous pouvons faire appel un transformers très peu sensible aux oilers le robuste skyler contrairement à la standardisation ici on ne soustrait pas nos données à la moyenne de chaque variable à la place on soustrait nos données à la médiane de chaque variable or vous le savez la médiane est beaucoup moins sensible aux haltes liars que peut l'être une moyenne ensuite au lieu de diviser par un écart type on va diviser

00:23:03	par l'inter quartile de nos données c'est à dire qu'on divise par l'écart qu'il y a dans nos données entre le troisième quartile et le premier quartile donc si on utilise ce robuste skyler pour transformer nos données en écrivant robust skyler - fit transforme de x alors on obtient des données qui sont bien plus faciles à exploiter que tout à l'heure leur but ce qu'est leur a donc permis d'effectuer une normalisation de nos données originel sans pour autant d'efforts mais nos données à cause de

00:23:40	certains outlier ce qu'il pourrait y avoir donc voilà pour les transformer de normalisation présent dans 5 langues comme vous pouvez le voir il en existe quelques autres notamment un transformers appelé normal either mais attention celui ci a la particularité de normaliser les lignes de votre dada 7 et non pas les colonnes sans entrer dans les détails c'est une opération qui peut être utile si vous faites une naturelle which processing et je ne vous conseille pas d'utiliser se transformer si vous

00:24:12	débutez en date à sainz ou en machine org à présent je vais rapidement passer en revue les autres transformer qu'on trouve dans le module prix processing tout d'abord nous avons polynomiale fingers qui est très utile pour créer des variables polynomiale à partir de nos variable existantes c'est ce qu'on appelle faire du féticheur engineering et ça permet de développer des modèles de machine learning plus riche et plus sophistiquées par exemple si nous désirons développer un modèle polynomiale 2° 2 à partir d'une seule

00:24:49	variable x le transformer polynomiale fishers va alors créer une colonne de billets égal à 1 une colonne x et une colonne x au carré ce qui permettra à notre machine de trouver les paramètres à b et c d'un modèle polynôme lf2 knicks égal à x car et plus bx plus sait on peut ainsi développer des modèles bien plus sophistiqués par exemple j'ai ici le nuage de points suivants et en développant un modèle de régression linéaire le résultat pour n'obtient n'est pas très satisfaisant mais si je crée de
INTERESSANT
00:25:25	nouvelles variables polynomiale à partir de mai variable existantes en créant par exemple un degré 3 et que j'utilise ces variables dans une solveur linéaire alors le résultat que je vais obtenir est beaucoup plus satisfaisant maintenant dans le cas où nous disposons de plusieurs variables le transformer polynomiale fishers va créer toutes les combinaisons de variables possible pour créer un polynôme a donc sur un vrai dataset ça peut être utile par exemple pour combiner ensemble la surface d'un

00:26:02	appartement avec le nombre de pièces qu'il ya dans cet appartement le fait de créer ainsi une nouvelle variable va parfois vous permettre de développer des modèles plus performants mais bien sûr il ne faut pas oublier de normaliser vos données après avoir utilisé polynomiale fishers voilà donc pour le transformer polynomiale fishers un autre type d'opération qu'on retrouve dans ces kits l'orne sont les opérations de transformation non linéaire comme hour transforme ou quant à elle transforme c'est de transformer

00:26:35	permettent de traiter nos données pour leur accorder une distribution plus normal ou gaussienne ce qui facilite encore une fois l'apprentissage d'un certain nombre de modèles de machines leur mire je ne vais pas rentrer dans les détails parce qu on touche ici à des notions mathématiques qui vont un peu au delà de cette vidéo mais je manquerai pas de le faire dans une autre vidéo qui sera un peu plus orienté vers les maths ensuite nous avons des transformers qui permettent de faire des opérations de

00:27:03	discrétisation c'est-à-dire découper une variable continu en plusieurs parties ce type d'opération peut être très utile pour créer automatiquement des catégories dans une variable comme par exemple différentes catégories d'âge dans une variable qui comprend différents âges pour ça il existe deux transformer le plus simple c'est ben heine riser qui permet de diviser une variable en deux catégories selon un seuil que vous définissez dans le transformer par exemple j'ai ici une variable x qui va de zéro jusqu'à 5 et

00:27:39	je peux utiliser beiner riser en disant qu'on va définir le seuil égale à 3 et tous qui sera inférieure à 3 sera converti en zéro et tout ce qui sera super 1 à 3 sera converti en sait il le concept est très simple l'autre transformers 2 discrétisation c'est gai bi nze 10 14 heures celui ci permet de faire exactement la même chose mais en découpant notre variable en plus que deux catégories voilà donc pour les opérations de discrétisation et pour finir dans le module de prix processing vous pouvez

00:28:13	voir un dernier transformers celui-là est en fait un transformers personnalisable vous pouvez rentrer n'importe quelle fonction à l'intérieur et ça va vous retourner une fonction de transformation voilà vous connaissez à présent tous les transformer du module prix processing ne s'acquittent l'orne si vous débutez en machine learning je vous conseille de retenir la belle anne code i want and co d'heure pour les opérations d'encodage mines max skyler standaard skyler pour les opérations de normalisation et éventuellement

00:28:44	polynomiale feature ans parce qu'il peut être sympa bon maintenant que vous connaissez tous et transformer il est temps de voir comment les combinés avec des estimateurs pour créer des modèles de machine learning vraiment performant pour ça nous allons utiliser une des classes les plus importantes de tous ces kits l'orne la classe paris plages [Musique] comme on l'a vu au début de la vidéo lorsqu'on développe un modèle de machine learning on utilise en premier lieu les données du trend 7 pour développer un ou

00:29:18	plusieurs transformer ce qui nous permet de traiter ces données pour ensuite entre année un estimateur une fois cette étape terminée on utilise nous transformer tels qu'ils ont été développés pour traiter les données du test cette donnée que l'on peut ensuite fournir à notre estimateur pour qu'il effectue ses prédictions si on tente de reproduire ce schéma dans python par exemple sur le data 7 des fleurs d'iris on obtient le code suivant on commence par diviser notre data 7 en deux parties 1 37 et 1 à 7 puis on crée

00:29:55	un transformers par exemple standard skyler qui nous sert à traiter nos données x-trail après quoi on développe notre estimateur par exemple un estimateur de la classe sgd classe i nférieure que l'on va développer avec la méthode fit fuite x trail qui a dit qu transformer et y traîne pour finir si on désire utiliser notre modèle pour faire des prédictions sur le test 7 nous allons commencer par transformer les données du tz7 à l'aide de notre skyler en utilisant la méthode transforme puis nous allons injecter ces données

00:30:32	qui ont été transformés ou dans la méthode predict de notre modèle maintenant il existe une façon beaucoup plus simple et efficace décrire ce code l'astuce c'est de regrouper notre transformer et notre estimateur dans une pipe line une chaîne de transformation on obtient ainsi un estimateur composite c'est-à-dire un estimateur composé de plusieurs éléments cette estimateur dispose comme tous les autres estimateur d'une méthode fit d'une méthode predict et d'une méthode score lorsqu'on utilise la méthode fit

00:31:06	tous les composants de cette estimateur vont utiliser leur méthode fit ainsi on développe nos transformer et notre estimateur d'un seul coup sûr une seule ligne de code de la même manière quand on utilise la méthode predict de cette estimateur composites alors tous les transformers vont transformer les données et l'estimateur qui se trouve en bout de chaîne va effectuer une prédiction donc dans notre cas pour écrire ce code sous forme de pipeline nous allons créer un modèle qui va être le résultat de la fonction make up lane

00:31:43	dans lequel nous aurons d'abord faire passer notre standard skyler après quoi nous allons pouvoir rajouter notre sgd classify heures à présent il suffit d'utiliser la méthode fit sur notre modèle donc fit x trail in great dane puis modèle points score ou bien modèle point predict pour commencer x test et on obtient exactement le même résultat le fait d'utiliser une pipe line pour regrouper nos transformer dans un seul objet présente plusieurs avantages premièrement c'est plus simple à lire c'est plus simple à utiliser

00:32:24	deuxièmement c'est plus sécurisé et troisièmement avoir une pipe line ça nous permet d'effectuer des opérations de cross validation sur l'ensemble notre de notre chaîne de transformation et qui dit crosby dyson dit forcément grid search cv et c'est là que les parties planes sont absolument génial vous pouvez optimiser toute une chaîne de transformation avec great search et tout ce que vous avez à faire c'est de définir votre pipe line de définir le nombre de split pour votre cros validation et de définir un

00:33:00	dictionnaire paramètres dans lequel vous allez identifier chaque paramètre de votre chaîne de transformation à optimiser donc démonstration nous allons apporter grid search cv et puis nous allons recréer un modèle avec la fonction make haiping dans laquelle il on pourra faire passer polynomiale switchers puis nous allons faire passer une normalisation de standaard skyler et nous allons utiliser sgd classify heures que nous allons au passage initialiser avec un golden state égal à zéro une fois notre 'pipeline' créé nous allons

00:33:36	pouvoir l'observer donc notre pipeline ressemble à ça on a les différentes étapes donc si on désire créer notre dictionnaire de paramètres on va créer par exemple à paramètres qui va gérer le nombre de degrés que l'on a dans polynomiale fishers donc on écrit polynomiale fishers underscore unscore de grille et ensuite on va dire qu'on va chercher entre 2,3 et 4 degrés par exemple maintenant un deuxième paramètre ça sera par exemple le paramètre pénalité de sgd classify heures donc on va prendre sgd classify heures on va écrire deux

00:34:18	fois ender score puis on va écrire pénalité donc maintenant qu'on a défini notre dictionnaire il ne reste plus qu'à créer notre grille de recherche great search tv dans lequel on faire passer notre modèle on va faire passer notre dictionnaire de paramètres et nous allons faire passer un nombre de split pour d'autres creusent validation par exemple 4 spitz à ce stade il ne reste plus qu'à utiliser notre grille de recherche vous qu'on va faire greed point fit x strain y trane si on entraîne tout ça on espère

00:34:54	qu'ils aient pas de problème apparemment n'y a pas eu de problème tout s'est bien passé et donc il ne reste plus qu'à voir quelles sont par exemple nos meilleurs paramètres et donc nos meilleurs paramètres sont 4 degrés avec le penalty l1 et donc on obtient une performance de 97% c'est pas mal franchement c'est pas mal maintenant vous vous demandez peut-être qu'elle serait la performance de notre modèle sans faire de procès si sans faire de normalisation sans faire tout ce qu'on a vu dans cette vidéo

00:35:25	lien en voyant ça tout de suite si on reprend la base de la bad c'est à dire qu créer un modèle sgd classify heures brendon state égal 0 l'histoire de partir avec les mêmes conditions initiales et que ce modèle on l'entraînent sur x trane y puis qu'on l'ait value sur x test in iraq tasdon donc nos données brutes sans faire de prix processing on obtient alors un score de 84 % donc je sais pas si vous vous rendez compte mais entre 84% et 97% ça fait une différence de 13% en date à sainz c'est énorme donc retenez bien ça en

00:36:13	machine learning le plus important c'est la qualité de vos données ainsi que le nombre de données et le prix pricing c'est la clé de la réussite croyez moi voilà c'est la fin de cette vidéo sur le prix processing avec s'acquitte l'orne je vais faire pas mal de vidéos bonus dans les prochains jours une vidéo sur les pipelines une vidéo sur les autres modules de prix processing de ces kits langues comme le module intune fincher sélection figure extraction donc n'hésitez pas à vous abonner si vous

00:36:43	voulez pas les louper pour vous exercer je vous invite à recréer la timeline que nous avons développée afin d'essayer de trouver quels seraient les transformers qui vous donne les meilleurs résultats ça peut être mini max ou bien robuste skyler n'hésitez pas à dire dans les commentaires quelles sont les solutions qui vous donne les meilleurs résultats ça serait intéressant de partager ça avec la communauté et si vous avez la moindre difficulté vous pouvez compter sur mon aide il suffit de poser une question en

00:37:11	commentaires ou bien de rejoindre notre communauté sur dix corps dur à plein de gens pour vous aider si la vidéo vous a plu merci lalique et et merci de la partager avec un ami quant à moi je vous dis à très vite pour la prochaine vidéo [Musique] [Musique]



00:00:00	bozo si vous avez vous montrer common develop a the pipeline's even say a Mexican grassy piranhas recap ability the data set hey Audrina stood on a data set a container - tip the viable a viable discreditable continue the viable we present a demo to Saddam buta develop a model dimensional ninja kappa phone pasado el vídeo de noir tow function the function make current transformer la function megacolon selector a linear function make union ooh Madonn situation canoe an example dataset rail ec r a

00:00:42	data set routine common belvoir natsu data set on a different oblivion example available continue a variable age' Deborah discrete common variable poklis a devotee contained EMU comrade Beria blue sex pokémon second desert rat a nutri dataset avec de femme transformer comparison a standard SCADA alloy before a fellow tree dono different corner supremum Pasco rooster not Caribbean except oku Dutra TDI Bobby Abreu numerator non redundant current sex Jana : class embarked in a even go to nineveh local Emma secured come country

00:01:24	in a pipeline avec assemble and estimator emma dante pipeline o una vida FERPA Sinatra data set on naughty authority to click alone poor Anthony not estimator may seem facade we are lost on our scale aha Azusa - Oh de nada de dentro pipeline Avenue Tony you neural biscone value demand a new jersey to route a blue X Z Daniel took no karana prefer in normalization Oh Papa transformer occurrence X knee recurrent class niraakar on Yahoo etc doc demo stations run execute cerca de mi hacen una manera

00:02:04	doc pol Azul o su problema Elva follow Kate Danaher pipeline a McCain is no cheaper men do da likkle oh no founded on a certain karana assessed on transformer IDO to occur on a do to transform a professor new zato Ichi Rosella function make current transformer new modulo compose do cyclin set function per module in a transformer key nova applique in function de transformation Cossacks certain : : : knowledge doc dos a function over a creole so formidable Lupe rostro de transformation Quixote effect rep

00:02:45	Alexandre innovation tostadas Cairo serie de la lista de to click or anubhava sudhi exit transformation by example Monica to death desolation in control selection a locker on our secure a Current Affair Kovac we're a la verga low fare dog in compiler set sevilla nuevo ke anu o transformer keaton transformer non pas du tableau entangle me in ik mode : doc come to transform improve not funny Ava committed feet in metallic transform a no so transform operation free passage to not retablo eggs a la vie no velocity

00:03:31	to loot a block mimic Molly : ash affair no a Petrova dosa tableau anunciador de news Evo - Nancy appraisal luigi de su transformer Donatello scum affair second Val Lewton easy no neutral pipeline a Laplace declare stone upscaler on tone Katella - second which is in standard scalar a travel the transformer Heaton : transformer doc very easy transformer Yukawa each regional pipeliner the vassal collect me confessor on oppose the clip do to transform a monk scoff a typic mono cleaner in biplane antipope religion people

00:04:17	complex Tsukuba fellow tree on Holi corona kiss on the table category ella ella current sunday viable image dr. Alexandra Assiniboine a la una lista de vera numeric con la viable poklis our affair amidst the variable categorical give accompany la : sex Nicole on deck in a current alone oh sweet a party and receive duelist de varela of approval it is a transformer make current transformer poor appliqué will premiere serie de transformation Suho numerical features free not Vasily Oh transformations oh no categorical no

00:05:05	categorical features SAV do transformation in vasiliki su for meadow pipeline dog news a new tree you know numerical pipeline on a signal categorical pipeline ASAP planar news on all additional juice - Lucy unequivocal numerical pipeline suburban at major pipeline kiba koneko example a simple in pewter I found on V live Aloha Mekons Odaiba darshan pre-hospital affair and standard scaler welcome Susie a on sweetie not categorical pipeliner Sava donkey at window pipeline o Nanak Allah news alone double eagle moment a simple in pewter

00:05:57	reputed original strategy he consists a hompless a level of Mekons paulo very critical economy strategy a girl most frequent ensuite approve of me a simple in pewter Hamed poison why not encoder to supremo Laughlin is a male in pipeline a portrait a llevarlo numeric in pipeline portrait deliverable categorical on computer precede a pipeline of approval lasers only McCallum transform a dog Sava do numerical pipeline vedette application a numerical features senado a categorical pipeline audit applicatio categorical

00:06:39	features toss our salad on a transformer compare example Apple a preprocessor preprocessor attagirl SOC ensuite investor pre-katrina type Raynaud's phenomenon quietly cannot role model tiger a mega piranha nanak air of efficacy and a bachina not Rob preprocessor free of foundation not SGA classifier no sweetie no Esprit model my Vita it's a Greg itza Sudoku too low table Alexandre Collins arrived in RAC c2 low table nonce a simple molecule on solid rock Anisa encompasses a Bruma not remodel at Offutt Manetti on trainee dog vasila

00:07:26	procedure has reported a piranha rusev is te keepa meditating de data set a diversion por que no se yo fellow tree on trove of a numeric evolvable category inferred reserve a facade redefinition Porsche tiptoe variable no pipeline set a deer on shondo transformation Co severe loss Rubio epifanio's erectus a piranha Donna function make quorum transformer savate Ohana a preprocessor a transformer que vous pouvez entre neova which is a feat Vic transform transform me oh dude oh Jesus a sample matado Vujicic Alamo so preprocessor don

00:08:09	votre pipeline final selfie deafening volume model the machine on a presen dopey direction zero prevent to set it alone yay nouvelle function dis Panamera door modular compose Sarah function make column selector set function a VAR a motor duty Pascal duper may do selection Evo VII habla mal Padova soma explicit on the cleaver Bombardier class speech for a Jewish prophet so steeper kizomba Tom Savini so tender vibro pasado Mara plus vous pouvez don said function McCallum sector and e key tip do viable

00:08:50	coolly selection as cassava to enviable tip numeric categorical vous pouvez memo choisir in viola solo para para para selection tool available no come on spar in certain seconds do character by example a troll ABC TV etc latitudinal composer poop which is a legume on this type include d-type exclude poo and occasionally agua to slavery a block is on duty numerically numerical future of 3d type include NP number attitude eggnog a pal example a category or deliver a sunny category loser a home plus a set list per mega

00:09:44	column select our own little attitude example D type excluder NP number Boozer ad section a to skinny part on oboe sim computer on a pedal a oh sweet Amparo complete uno de kado Johanna model key on notice you like a cockroach conversation about me totally Corona numerical on cracker on big class SBS part of our tourist emblem a monopoly category beside sex on Bart class by Tuscon a vapor selection a palpable except nouvelle function a remote reservoir so to non-corrosive a decent and over a thousand data set to

00:10:28	facilities evolve our hostel a logic 0 Pavan dodo psychic Ron Simmons of a Paco Momotaro psychic learner news of a backseat example in Sophie de Vera Verto terminal anaconda Prieta Bay Conda update ta ta all the squad reconcider FL becomes a server goober metal to metal to leap occasion who jano an akuna chose k APPA pre-party poor Phineas's video a very common credit pipelines parle avec la function met Union on FA risk a presence on to Lafayette pipelines Amitabha taboo pre-service formal okay a sincere new

00:11:10	Transformer may a lexical impairment ready type rain he met a parallel transform a human Laura tjuta ensemble doses le mme tableau componentry Tusa VC Prada an example a prison planet is no sample more la : Allah Allah Quran fair do not ho dataset reporter la function make human the pyramid Allah psychic Lana I know set function Toscana FSA don't don't hey Li transformer Granville avoir unparallel bulk per example dzongkha Ducati use of only the stone of dizzy see to varriya noga genomically main

00:11:53	Union standard scalar a novel dosa move on Eagleman discrete easy sido variable for example of o'clock class by Nasser voici ma dong dong in pipeline palenik populist rasullah variable pipeline you know sweet said pipeline amia super cheesy effect limited feet feet transform etc Dora Kelvin on campus a numerical features exclusive a new botany say a vegetal key concern cattle : accepted a to difficile allele may see some primal leads Muslim verb to Neha circuit Hollander lineage 2030 on cat who : good leader Kemal : assume the

00:12:39	colon committee standard easy of external scalar e li didn't occur on the motor blows holy quran community discrete easy awake by Nigel voila I beg to salvage that diesel ninja Pablo to create the pipelines as a sophisticated psychical supported sutras rutila Sujit on data engineer on data scientist it Hawaii short cloud ekkuva Kuzma to deploy pollution amudha lo hace sophisticated boos oh no Pamela preprocessing diocese a voltage our solution is a actual Monday pipeline and Romania um okay you

00:13:15	do not Romania is Deepa Allison hakuna matata hakuna saw streak lucas duda community cinnamon callaway the kitchen this is Deepa hours one who not who suffer discernible data scientist Erica can approve a community visit upper atmospheric CPC we deserve a Muslim Neha Iowa polymer mukasam Ixia Antipolis look on you see the video with the premise invisible a methodology caught emergency at rabbit paw report news [Music] [Applause] [Music] [Applause] [Music]

